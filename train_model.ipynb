{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfd1732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "158/158 [==============================] - 10s 33ms/step - loss: 0.0767 - auc: 0.0000e+00 - accuracy: 0.9856 - val_loss: 5.7681e-05 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "158/158 [==============================] - 3s 20ms/step - loss: 6.5377e-05 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 2.2888e-05 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "158/158 [==============================] - 3s 20ms/step - loss: 3.2573e-05 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.2172e-05 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "157/158 [============================>.] - ETA: 0s - loss: 2.0005e-05 - auc: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "158/158 [==============================] - 3s 21ms/step - loss: 1.9993e-05 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 7.6722e-06 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "158/158 [==============================] - 3s 20ms/step - loss: 1.4666e-05 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 6.2895e-06 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 6/50\n",
      "158/158 [==============================] - 3s 21ms/step - loss: 1.2225e-05 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.1984e-06 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 7/50\n",
      "157/158 [============================>.] - ETA: 0s - loss: 1.0278e-05 - auc: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "158/158 [==============================] - 3s 22ms/step - loss: 1.0293e-05 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 4.3443e-06 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 8/50\n",
      "158/158 [==============================] - 3s 21ms/step - loss: 9.0125e-06 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 3.9773e-06 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 9/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 8.6061e-06 - auc: 0.0000e+00 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 1.\n",
      "158/158 [==============================] - 3s 21ms/step - loss: 8.6061e-06 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 3.6225e-06 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 9: early stopping\n",
      "Test evaluation: [5.783038432127796e-05, 0.0, 1.0]\n",
      "Saved lstm_migraine_model.h5 and scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "# train_model.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "CSV_PATH = \"synthetic_wsht_weather_migraine_prob_600days_hourly_FIXED.csv\"\n",
    "FEATURES = [\"workload_0_10\", \"stress_0_10\", \"hrv_rmssd_ms\"]\n",
    "TARGET_LABEL = \"migraine_prob_next_hour\"\n",
    "SEQ_LEN = 24\n",
    "RANDOM_SEED = 123\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "df = pd.read_csv(CSV_PATH, parse_dates=[\"timestamp\"])\n",
    "df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "df = df[~df[TARGET_LABEL].isna()].reset_index(drop=True)\n",
    "df[TARGET_LABEL] = df[TARGET_LABEL].astype(int)\n",
    "\n",
    "def build_sequences(frame, features, seq_len):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(frame[features])\n",
    "    joblib.dump(scaler, \"scaler.pkl\")  # save scaler\n",
    "\n",
    "    feat = scaler.transform(frame[features])\n",
    "    y = frame[TARGET_LABEL].to_numpy().astype(int)\n",
    "    X_list, y_list = [], []\n",
    "    for end in range(seq_len, len(frame)):\n",
    "        start = end - seq_len\n",
    "        X_list.append(feat[start:end, :])\n",
    "        y_list.append(y[end])\n",
    "    return np.array(X_list), np.array(y_list)\n",
    "\n",
    "X, y = build_sequences(df, FEATURES, SEQ_LEN)\n",
    "\n",
    "# split chronologically\n",
    "N = len(X)\n",
    "train_n = int(N * 0.7)\n",
    "val_n   = int(N * 0.15)\n",
    "X_train, y_train = X[:train_n], y[:train_n]\n",
    "X_val,   y_val   = X[train_n:train_n+val_n], y[train_n:train_n+val_n]\n",
    "X_test,  y_test  = X[train_n+val_n:], y[train_n+val_n:]\n",
    "\n",
    "# Diagnostic: Check class distribution\n",
    "print(\"=\" * 60)\n",
    "print(\"CLASS DISTRIBUTION DIAGNOSTIC\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count positive samples\n",
    "train_pos = y_train.sum()\n",
    "val_pos = y_val.sum()\n",
    "test_pos = y_test.sum()\n",
    "\n",
    "print(f\"\\nTraining set:\")\n",
    "print(f\"  Total: {len(y_train)}, Positive (migraine=1): {train_pos} ({100*train_pos/len(y_train):.2f}%)\")\n",
    "\n",
    "print(f\"\\nValidation set:\")\n",
    "print(f\"  Total: {len(y_val)}, Positive (migraine=1): {val_pos} ({100*val_pos/len(y_val):.2f}%)\")\n",
    "\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  Total: {len(y_test)}, Positive (migraine=1): {test_pos} ({100*test_pos/len(y_test):.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "if val_pos > 10 and test_pos > 10:\n",
    "    print(\"✓ GOOD: Enough positive cases to train effectively\")\n",
    "else:\n",
    "    print(\"✗ WARNING: Not enough positive cases. Regenerate CSV with higher coefficients!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def make_model(input_len, input_dim):\n",
    "    inputs = keras.Input(shape=(input_len, input_dim))\n",
    "    x = layers.Masking(mask_value=0.0)(inputs)\n",
    "    x = layers.LSTM(64)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(32, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(1e-3),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[keras.metrics.AUC(name=\"auc\"), \"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = make_model(SEQ_LEN, len(FEATURES))\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_auc\", mode=\"max\", factor=0.5, patience=3, verbose=1),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=8, restore_best_weights=True, verbose=1),\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    class_weight={0: 1.0, 1: 2.5},\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "print(\"Test evaluation:\", model.evaluate(X_test, y_test, verbose=0))\n",
    "model.save(\"lstm_migraine_model.h5\")\n",
    "print(\"Saved lstm_migraine_model.h5 and scaler.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "junction-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
