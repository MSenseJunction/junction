{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bdfd1732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CLASS DISTRIBUTION DIAGNOSTIC\n",
      "============================================================\n",
      "\n",
      "Training set:\n",
      "  Total: 10062, Positive (migraine=1): 0.0 (0.00%)\n",
      "\n",
      "Validation set:\n",
      "  Total: 2156, Positive (migraine=1): 0.0 (0.00%)\n",
      "\n",
      "Test set:\n",
      "  Total: 2157, Positive (migraine=1): 0.0 (0.00%)\n",
      "\n",
      "============================================================\n",
      "✗ WARNING: Not enough positive cases. Regenerate CSV with higher coefficients!\n",
      "============================================================\n",
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "158/158 [==============================] - 11s 36ms/step - loss: 0.0271 - auc: 0.0000e+00 - accuracy: 0.9823 - val_loss: 1.3667e-05 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "158/158 [==============================] - 11s 36ms/step - loss: 0.0271 - auc: 0.0000e+00 - accuracy: 0.9823 - val_loss: 1.3667e-05 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "158/158 [==============================] - 4s 23ms/step - loss: 2.2273e-05 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 4.3701e-06 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "158/158 [==============================] - 4s 23ms/step - loss: 2.2273e-05 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 4.3701e-06 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "158/158 [==============================] - 4s 23ms/step - loss: 9.2204e-06 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.9676e-06 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "158/158 [==============================] - 4s 23ms/step - loss: 9.2204e-06 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.9676e-06 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 5.2347e-06 - auc: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "158/158 [==============================] - 4s 23ms/step - loss: 5.2347e-06 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.0777e-06 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "158/158 [==============================] - 4s 23ms/step - loss: 5.2347e-06 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.0777e-06 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "158/158 [==============================] - 4s 24ms/step - loss: 3.5006e-06 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.2338e-07 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 6/50\n",
      "158/158 [==============================] - 4s 24ms/step - loss: 3.5006e-06 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.2338e-07 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 6/50\n",
      "158/158 [==============================] - 4s 24ms/step - loss: 3.0345e-06 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 6.2905e-07 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 7/50\n",
      "158/158 [==============================] - 4s 24ms/step - loss: 3.0345e-06 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 6.2905e-07 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 7/50\n",
      "157/158 [============================>.] - ETA: 0s - loss: 2.3751e-06 - auc: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "158/158 [==============================] - 4s 23ms/step - loss: 2.3805e-06 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 4.8818e-07 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "158/158 [==============================] - 4s 23ms/step - loss: 2.3805e-06 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 4.8818e-07 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 8/50\n",
      "158/158 [==============================] - 4s 24ms/step - loss: 2.0027e-06 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 4.3129e-07 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 9/50\n",
      "158/158 [==============================] - 4s 24ms/step - loss: 2.0027e-06 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 4.3129e-07 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 9/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 1.7873e-06 - auc: 0.0000e+00 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 1.\n",
      "158/158 [==============================] - 4s 24ms/step - loss: 1.7873e-06 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 3.7879e-07 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 2.5000e-04\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "158/158 [==============================] - 4s 24ms/step - loss: 1.7873e-06 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 3.7879e-07 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 9: early stopping\n",
      "Epoch 9: early stopping\n",
      "Test evaluation: [1.4417416423384566e-05, 0.0, 1.0]\n",
      "Saved lstm_migraine_model.h5 and scaler.pkl\n",
      "Test evaluation: [1.4417416423384566e-05, 0.0, 1.0]\n",
      "Saved lstm_migraine_model.h5 and scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "# train_model.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "CSV_PATH = \"synthetic_wsht_weather_migraine_prob_600days_hourly_FIXED.csv\"\n",
    "FEATURES = [\"workload_0_10\", \"stress_0_10\", \"hrv_rmssd_ms\"]\n",
    "TARGET_LABEL = \"migraine_prob_next_hour\"\n",
    "SEQ_LEN = 24\n",
    "RANDOM_SEED = 123\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "df = pd.read_csv(CSV_PATH, parse_dates=[\"timestamp\"])\n",
    "df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "df = df[~df[TARGET_LABEL].isna()].reset_index(drop=True)\n",
    "df[TARGET_LABEL] = df[TARGET_LABEL].astype(int)\n",
    "\n",
    "def build_sequences(frame, features, seq_len):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(frame[features])\n",
    "    joblib.dump(scaler, \"scaler.pkl\")  # save scaler\n",
    "\n",
    "    feat = scaler.transform(frame[features])\n",
    "    y = frame[TARGET_LABEL].astype(\"float32\")\n",
    "    X_list, y_list = [], []\n",
    "    for end in range(seq_len, len(frame)):\n",
    "        start = end - seq_len\n",
    "        X_list.append(feat[start:end, :])\n",
    "        y_list.append(y[end])\n",
    "    return np.array(X_list), np.array(y_list)\n",
    "\n",
    "X, y = build_sequences(df, FEATURES, SEQ_LEN)\n",
    "\n",
    "# split chronologically\n",
    "N = len(X)\n",
    "train_n = int(N * 0.7)\n",
    "val_n   = int(N * 0.15)\n",
    "X_train, y_train = X[:train_n], y[:train_n]\n",
    "X_val,   y_val   = X[train_n:train_n+val_n], y[train_n:train_n+val_n]\n",
    "X_test,  y_test  = X[train_n+val_n:], y[train_n+val_n:]\n",
    "\n",
    "# Diagnostic: Check class distribution\n",
    "print(\"=\" * 60)\n",
    "print(\"CLASS DISTRIBUTION DIAGNOSTIC\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count positive samples\n",
    "train_pos = y_train.sum()\n",
    "val_pos = y_val.sum()\n",
    "test_pos = y_test.sum()\n",
    "\n",
    "print(f\"\\nTraining set:\")\n",
    "print(f\"  Total: {len(y_train)}, Positive (migraine=1): {train_pos} ({100*train_pos/len(y_train):.2f}%)\")\n",
    "\n",
    "print(f\"\\nValidation set:\")\n",
    "print(f\"  Total: {len(y_val)}, Positive (migraine=1): {val_pos} ({100*val_pos/len(y_val):.2f}%)\")\n",
    "\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  Total: {len(y_test)}, Positive (migraine=1): {test_pos} ({100*test_pos/len(y_test):.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "if val_pos > 10 and test_pos > 10:\n",
    "    print(\"✓ GOOD: Enough positive cases to train effectively\")\n",
    "else:\n",
    "    print(\"✗ WARNING: Not enough positive cases. Regenerate CSV with higher coefficients!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def make_model(input_len, input_dim):\n",
    "    inputs = keras.Input(shape=(input_len, input_dim))\n",
    "    x = layers.Masking(mask_value=0.0)(inputs)\n",
    "    x = layers.LSTM(64)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(32, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(1e-3),\n",
    "        loss=\"mse\",\n",
    "        metrics=[keras.metrics.AUC(name=\"auc\"), \"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = make_model(SEQ_LEN, len(FEATURES))\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_auc\", mode=\"max\", factor=0.5, patience=3, verbose=1),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=8, restore_best_weights=True, verbose=1),\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "print(\"Test evaluation:\", model.evaluate(X_test, y_test, verbose=0))\n",
    "model.save(\"lstm_migraine_model.h5\")\n",
    "print(\"Saved lstm_migraine_model.h5 and scaler.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee8a5e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\peace\\anaconda3\\envs\\junction-cpu\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "HIGH RISK SCENARIO\n",
      "============================================================\n",
      "Features: High stress (8.7-9.5/10), High workload (7.9-8.6/10), Low HRV (24.5-26.2 ms)\n",
      "Migraine probability: 0.0000 (0.00%)\n",
      "\n",
      "============================================================\n",
      "LOW RISK SCENARIO\n",
      "============================================================\n",
      "Features: Low stress (2.2-2.6/10), Low workload (2.3-2.7/10), High HRV (64.0-66.2 ms)\n",
      "Migraine probability: 0.0000 (0.00%)\n",
      "\n",
      "============================================================\n",
      "MEDIUM RISK SCENARIO\n",
      "============================================================\n",
      "Features: Medium stress (5.2-5.6/10), Medium workload (5.3-5.7/10), Medium HRV (44.0-46.2 ms)\n",
      "Migraine probability: 0.0000 (0.00%)\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "High risk:   0.0000\n",
      "Medium risk: 0.0000\n",
      "Low risk:    0.0000\n",
      "\n",
      "Expected: High risk > Medium risk > Low risk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\peace\\anaconda3\\envs\\junction-cpu\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\peace\\anaconda3\\envs\\junction-cpu\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Test inference with high-risk features\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load model & scaler\n",
    "model = keras.models.load_model(\"lstm_migraine_model.h5\")\n",
    "scaler = joblib.load(\"scaler.pkl\")\n",
    "\n",
    "# ============================================================\n",
    "# HIGH RISK SCENARIO: High stress, high workload, low HRV\n",
    "# ============================================================\n",
    "high_risk_window = np.array([\n",
    "    [9.5, 8.5, 25.0],   # Hour 0: Very high stress/workload, low HRV\n",
    "    [9.2, 8.3, 26.0],   # Hour 1\n",
    "    [9.0, 8.1, 24.5],   # Hour 2\n",
    "    [8.8, 8.0, 25.5],   # Hour 3\n",
    "    [8.5, 7.9, 26.0],   # Hour 4\n",
    "    [9.1, 8.2, 25.2],   # Hour 5\n",
    "    [9.3, 8.4, 24.8],   # Hour 6\n",
    "    [9.0, 8.3, 25.5],   # Hour 7\n",
    "    [8.9, 8.1, 26.1],   # Hour 8\n",
    "    [9.2, 8.5, 24.9],   # Hour 9\n",
    "    [9.4, 8.6, 25.3],   # Hour 10\n",
    "    [9.1, 8.2, 25.8],   # Hour 11\n",
    "    [8.7, 7.9, 26.2],   # Hour 12\n",
    "    [9.0, 8.1, 25.1],   # Hour 13\n",
    "    [9.3, 8.4, 24.7],   # Hour 14\n",
    "    [8.9, 8.0, 25.9],   # Hour 15\n",
    "    [9.2, 8.3, 25.4],   # Hour 16\n",
    "    [9.1, 8.2, 26.0],   # Hour 17\n",
    "    [8.8, 7.9, 25.6],   # Hour 18\n",
    "    [9.4, 8.5, 24.8],   # Hour 19\n",
    "    [9.0, 8.1, 25.3],   # Hour 20\n",
    "    [8.9, 8.0, 26.1],   # Hour 21\n",
    "    [9.3, 8.4, 25.0],   # Hour 22\n",
    "    [9.2, 8.3, 25.5],   # Hour 23\n",
    "])\n",
    "\n",
    "# Normalize using the training scaler\n",
    "high_risk_scaled = scaler.transform(high_risk_window)\n",
    "\n",
    "# Add batch dimension\n",
    "high_risk_scaled = high_risk_scaled[np.newaxis, :, :]\n",
    "\n",
    "# Predict\n",
    "high_risk_pred = model.predict(high_risk_scaled, verbose=0)\n",
    "print(\"=\" * 60)\n",
    "print(\"HIGH RISK SCENARIO\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Features: High stress (8.7-9.5/10), High workload (7.9-8.6/10), Low HRV (24.5-26.2 ms)\")\n",
    "print(f\"Migraine probability: {high_risk_pred[0][0]:.4f} ({100*high_risk_pred[0][0]:.2f}%)\")\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# LOW RISK SCENARIO: Low stress, low workload, high HRV\n",
    "# ============================================================\n",
    "low_risk_window = np.array([\n",
    "    [2.5, 2.5, 65.0],   # Hour 0: Low stress/workload, high HRV\n",
    "    [2.3, 2.4, 64.0],   # Hour 1\n",
    "    [2.4, 2.6, 65.5],   # Hour 2\n",
    "    [2.6, 2.5, 66.0],   # Hour 3\n",
    "    [2.2, 2.3, 64.5],   # Hour 4\n",
    "    [2.4, 2.5, 65.2],   # Hour 5\n",
    "    [2.3, 2.4, 65.8],   # Hour 6\n",
    "    [2.5, 2.6, 64.9],   # Hour 7\n",
    "    [2.4, 2.5, 65.3],   # Hour 8\n",
    "    [2.6, 2.7, 66.1],   # Hour 9\n",
    "    [2.3, 2.4, 64.6],   # Hour 10\n",
    "    [2.4, 2.5, 65.4],   # Hour 11\n",
    "    [2.5, 2.6, 65.9],   # Hour 12\n",
    "    [2.2, 2.3, 64.2],   # Hour 13\n",
    "    [2.3, 2.4, 65.7],   # Hour 14\n",
    "    [2.4, 2.5, 66.0],   # Hour 15\n",
    "    [2.6, 2.7, 65.1],   # Hour 16\n",
    "    [2.3, 2.4, 64.8],   # Hour 17\n",
    "    [2.5, 2.6, 65.5],   # Hour 18\n",
    "    [2.4, 2.5, 66.2],   # Hour 19\n",
    "    [2.2, 2.3, 64.3],   # Hour 20\n",
    "    [2.3, 2.4, 65.6],   # Hour 21\n",
    "    [2.5, 2.6, 65.8],   # Hour 22\n",
    "    [2.4, 2.5, 66.1],   # Hour 23\n",
    "])\n",
    "\n",
    "# Normalize using the training scaler\n",
    "low_risk_scaled = scaler.transform(low_risk_window)\n",
    "\n",
    "# Add batch dimension\n",
    "low_risk_scaled = low_risk_scaled[np.newaxis, :, :]\n",
    "\n",
    "# Predict\n",
    "low_risk_pred = model.predict(low_risk_scaled, verbose=0)\n",
    "print(\"=\" * 60)\n",
    "print(\"LOW RISK SCENARIO\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Features: Low stress (2.2-2.6/10), Low workload (2.3-2.7/10), High HRV (64.0-66.2 ms)\")\n",
    "print(f\"Migraine probability: {low_risk_pred[0][0]:.4f} ({100*low_risk_pred[0][0]:.2f}%)\")\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# MEDIUM RISK SCENARIO: Moderate values\n",
    "# ============================================================\n",
    "medium_risk_window = np.array([\n",
    "    [5.5, 5.5, 45.0],   # Hour 0: Medium everything\n",
    "    [5.3, 5.4, 44.0],   # Hour 1\n",
    "    [5.4, 5.6, 45.5],   # Hour 2\n",
    "    [5.6, 5.5, 46.0],   # Hour 3\n",
    "    [5.2, 5.3, 44.5],   # Hour 4\n",
    "    [5.4, 5.5, 45.2],   # Hour 5\n",
    "    [5.3, 5.4, 45.8],   # Hour 6\n",
    "    [5.5, 5.6, 44.9],   # Hour 7\n",
    "    [5.4, 5.5, 45.3],   # Hour 8\n",
    "    [5.6, 5.7, 46.1],   # Hour 9\n",
    "    [5.3, 5.4, 44.6],   # Hour 10\n",
    "    [5.4, 5.5, 45.4],   # Hour 11\n",
    "    [5.5, 5.6, 45.9],   # Hour 12\n",
    "    [5.2, 5.3, 44.2],   # Hour 13\n",
    "    [5.3, 5.4, 45.7],   # Hour 14\n",
    "    [5.4, 5.5, 46.0],   # Hour 15\n",
    "    [5.6, 5.7, 45.1],   # Hour 16\n",
    "    [5.3, 5.4, 44.8],   # Hour 17\n",
    "    [5.5, 5.6, 45.5],   # Hour 18\n",
    "    [5.4, 5.5, 46.2],   # Hour 19\n",
    "    [5.2, 5.3, 44.3],   # Hour 20\n",
    "    [5.3, 5.4, 45.6],   # Hour 21\n",
    "    [5.5, 5.6, 45.8],   # Hour 22\n",
    "    [5.4, 5.5, 46.1],   # Hour 23\n",
    "])\n",
    "\n",
    "# Normalize using the training scaler\n",
    "medium_risk_scaled = scaler.transform(medium_risk_window)\n",
    "\n",
    "# Add batch dimension\n",
    "medium_risk_scaled = medium_risk_scaled[np.newaxis, :, :]\n",
    "\n",
    "# Predict\n",
    "medium_risk_pred = model.predict(medium_risk_scaled, verbose=0)\n",
    "print(\"=\" * 60)\n",
    "print(\"MEDIUM RISK SCENARIO\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Features: Medium stress (5.2-5.6/10), Medium workload (5.3-5.7/10), Medium HRV (44.0-46.2 ms)\")\n",
    "print(f\"Migraine probability: {medium_risk_pred[0][0]:.4f} ({100*medium_risk_pred[0][0]:.2f}%)\")\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# Summary comparison\n",
    "# ============================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"High risk:   {high_risk_pred[0][0]:.4f}\")\n",
    "print(f\"Medium risk: {medium_risk_pred[0][0]:.4f}\")\n",
    "print(f\"Low risk:    {low_risk_pred[0][0]:.4f}\")\n",
    "print()\n",
    "print(\"Expected: High risk > Medium risk > Low risk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "junction-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
