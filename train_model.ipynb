{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bdfd1732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CLASS DISTRIBUTION DIAGNOSTIC\n",
      "============================================================\n",
      "\n",
      "Training set:\n",
      "  Total: 10062, Positive (migraine=1): 0.0 (0.00%)\n",
      "\n",
      "Validation set:\n",
      "  Total: 2156, Positive (migraine=1): 0.0 (0.00%)\n",
      "\n",
      "Test set:\n",
      "  Total: 2157, Positive (migraine=1): 0.0 (0.00%)\n",
      "\n",
      "============================================================\n",
      "✗ WARNING: Not enough positive cases. Regenerate CSV with higher coefficients!\n",
      "============================================================\n",
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "158/158 [==============================] - 11s 36ms/step - loss: 0.0271 - auc: 0.0000e+00 - accuracy: 0.9823 - val_loss: 1.3667e-05 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "158/158 [==============================] - 11s 36ms/step - loss: 0.0271 - auc: 0.0000e+00 - accuracy: 0.9823 - val_loss: 1.3667e-05 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "158/158 [==============================] - 4s 23ms/step - loss: 2.2273e-05 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 4.3701e-06 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "158/158 [==============================] - 4s 23ms/step - loss: 2.2273e-05 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 4.3701e-06 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "158/158 [==============================] - 4s 23ms/step - loss: 9.2204e-06 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.9676e-06 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "158/158 [==============================] - 4s 23ms/step - loss: 9.2204e-06 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.9676e-06 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 5.2347e-06 - auc: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "158/158 [==============================] - 4s 23ms/step - loss: 5.2347e-06 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.0777e-06 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "158/158 [==============================] - 4s 23ms/step - loss: 5.2347e-06 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.0777e-06 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "158/158 [==============================] - 4s 24ms/step - loss: 3.5006e-06 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.2338e-07 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 6/50\n",
      "158/158 [==============================] - 4s 24ms/step - loss: 3.5006e-06 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.2338e-07 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 6/50\n",
      "158/158 [==============================] - 4s 24ms/step - loss: 3.0345e-06 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 6.2905e-07 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 7/50\n",
      "158/158 [==============================] - 4s 24ms/step - loss: 3.0345e-06 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 6.2905e-07 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 7/50\n",
      "157/158 [============================>.] - ETA: 0s - loss: 2.3751e-06 - auc: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "158/158 [==============================] - 4s 23ms/step - loss: 2.3805e-06 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 4.8818e-07 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "158/158 [==============================] - 4s 23ms/step - loss: 2.3805e-06 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 4.8818e-07 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 8/50\n",
      "158/158 [==============================] - 4s 24ms/step - loss: 2.0027e-06 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 4.3129e-07 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 9/50\n",
      "158/158 [==============================] - 4s 24ms/step - loss: 2.0027e-06 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 4.3129e-07 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 9/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 1.7873e-06 - auc: 0.0000e+00 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 1.\n",
      "158/158 [==============================] - 4s 24ms/step - loss: 1.7873e-06 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 3.7879e-07 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 2.5000e-04\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "158/158 [==============================] - 4s 24ms/step - loss: 1.7873e-06 - auc: 0.0000e+00 - accuracy: 1.0000 - val_loss: 3.7879e-07 - val_auc: 0.0000e+00 - val_accuracy: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 9: early stopping\n",
      "Epoch 9: early stopping\n",
      "Test evaluation: [1.4417416423384566e-05, 0.0, 1.0]\n",
      "Saved lstm_migraine_model.h5 and scaler.pkl\n",
      "Test evaluation: [1.4417416423384566e-05, 0.0, 1.0]\n",
      "Saved lstm_migraine_model.h5 and scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "# train_model.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "CSV_PATH = \"synthetic_wsht_weather_migraine_prob_600days_hourly_FIXED.csv\"\n",
    "FEATURES = [\"workload_0_10\", \"stress_0_10\", \"hrv_rmssd_ms\"]\n",
    "TARGET_LABEL = \"migraine_prob_next_hour\"\n",
    "SEQ_LEN = 24\n",
    "RANDOM_SEED = 123\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "df = pd.read_csv(CSV_PATH, parse_dates=[\"timestamp\"])\n",
    "df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "df = df[~df[TARGET_LABEL].isna()].reset_index(drop=True)\n",
    "df[TARGET_LABEL] = df[TARGET_LABEL].astype(int)\n",
    "\n",
    "def build_sequences(frame, features, seq_len):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(frame[features])\n",
    "    joblib.dump(scaler, \"scaler.pkl\")  # save scaler\n",
    "\n",
    "    feat = scaler.transform(frame[features])\n",
    "    y = frame[TARGET_LABEL].astype(\"float32\")\n",
    "    X_list, y_list = [], []\n",
    "    for end in range(seq_len, len(frame)):\n",
    "        start = end - seq_len\n",
    "        X_list.append(feat[start:end, :])\n",
    "        y_list.append(y[end])\n",
    "    return np.array(X_list), np.array(y_list)\n",
    "\n",
    "X, y = build_sequences(df, FEATURES, SEQ_LEN)\n",
    "\n",
    "# split chronologically\n",
    "N = len(X)\n",
    "train_n = int(N * 0.7)\n",
    "val_n   = int(N * 0.15)\n",
    "X_train, y_train = X[:train_n], y[:train_n]\n",
    "X_val,   y_val   = X[train_n:train_n+val_n], y[train_n:train_n+val_n]\n",
    "X_test,  y_test  = X[train_n+val_n:], y[train_n+val_n:]\n",
    "\n",
    "# Diagnostic: Check class distribution\n",
    "print(\"=\" * 60)\n",
    "print(\"CLASS DISTRIBUTION DIAGNOSTIC\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count positive samples\n",
    "train_pos = y_train.sum()\n",
    "val_pos = y_val.sum()\n",
    "test_pos = y_test.sum()\n",
    "\n",
    "print(f\"\\nTraining set:\")\n",
    "print(f\"  Total: {len(y_train)}, Positive (migraine=1): {train_pos} ({100*train_pos/len(y_train):.2f}%)\")\n",
    "\n",
    "print(f\"\\nValidation set:\")\n",
    "print(f\"  Total: {len(y_val)}, Positive (migraine=1): {val_pos} ({100*val_pos/len(y_val):.2f}%)\")\n",
    "\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  Total: {len(y_test)}, Positive (migraine=1): {test_pos} ({100*test_pos/len(y_test):.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "if val_pos > 10 and test_pos > 10:\n",
    "    print(\"✓ GOOD: Enough positive cases to train effectively\")\n",
    "else:\n",
    "    print(\"✗ WARNING: Not enough positive cases. Regenerate CSV with higher coefficients!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def make_model(input_len, input_dim):\n",
    "    inputs = keras.Input(shape=(input_len, input_dim))\n",
    "    x = layers.Masking(mask_value=0.0)(inputs)\n",
    "    x = layers.LSTM(64)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(32, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(1e-3),\n",
    "        loss=\"mse\",\n",
    "        metrics=[keras.metrics.AUC(name=\"auc\"), \"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = make_model(SEQ_LEN, len(FEATURES))\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_auc\", mode=\"max\", factor=0.5, patience=3, verbose=1),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=8, restore_best_weights=True, verbose=1),\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "print(\"Test evaluation:\", model.evaluate(X_test, y_test, verbose=0))\n",
    "model.save(\"lstm_migraine_model.h5\")\n",
    "print(\"Saved lstm_migraine_model.h5 and scaler.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee8a5e15",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at lstm_migraine_model.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load model & scaler\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlstm_migraine_model.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m scaler \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaler.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# HIGH RISK SCENARIO: High stress, high workload, low HRV\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\peace\\anaconda3\\envs\\junction-cpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\peace\\anaconda3\\envs\\junction-cpu\\lib\\site-packages\\keras\\saving\\save.py:226\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(filepath_str):\n\u001b[1;32m--> 226\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[0;32m    227\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo file or directory found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    228\u001b[0m         )\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(filepath_str):\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_load\u001b[38;5;241m.\u001b[39mload(\n\u001b[0;32m    232\u001b[0m             filepath_str, \u001b[38;5;28mcompile\u001b[39m, options\n\u001b[0;32m    233\u001b[0m         )\n",
      "\u001b[1;31mOSError\u001b[0m: No file or directory found at lstm_migraine_model.h5"
     ]
    }
   ],
   "source": [
    "# Test inference with high-risk features\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load model & scaler\n",
    "model = keras.models.load_model(\"lstm_migraine_model.h5\")\n",
    "scaler = joblib.load(\"scaler.pkl\")\n",
    "\n",
    "# ============================================================\n",
    "# HIGH RISK SCENARIO: High stress, high workload, low HRV\n",
    "# ============================================================\n",
    "high_risk_window = np.array([\n",
    "    [9.5, 8.5, 25.0],   # Hour 0: Very high stress/workload, low HRV\n",
    "    [9.2, 8.3, 26.0],   # Hour 1\n",
    "    [9.0, 8.1, 24.5],   # Hour 2\n",
    "    [8.8, 8.0, 25.5],   # Hour 3\n",
    "    [8.5, 7.9, 26.0],   # Hour 4\n",
    "    [9.1, 8.2, 25.2],   # Hour 5\n",
    "    [9.3, 8.4, 24.8],   # Hour 6\n",
    "    [9.0, 8.3, 25.5],   # Hour 7\n",
    "    [8.9, 8.1, 26.1],   # Hour 8\n",
    "    [9.2, 8.5, 24.9],   # Hour 9\n",
    "    [9.4, 8.6, 25.3],   # Hour 10\n",
    "    [9.1, 8.2, 25.8],   # Hour 11\n",
    "    [8.7, 7.9, 26.2],   # Hour 12\n",
    "    [9.0, 8.1, 25.1],   # Hour 13\n",
    "    [9.3, 8.4, 24.7],   # Hour 14\n",
    "    [8.9, 8.0, 25.9],   # Hour 15\n",
    "    [9.2, 8.3, 25.4],   # Hour 16\n",
    "    [9.1, 8.2, 26.0],   # Hour 17\n",
    "    [8.8, 7.9, 25.6],   # Hour 18\n",
    "    [9.4, 8.5, 24.8],   # Hour 19\n",
    "    [9.0, 8.1, 25.3],   # Hour 20\n",
    "    [8.9, 8.0, 26.1],   # Hour 21\n",
    "    [9.3, 8.4, 25.0],   # Hour 22\n",
    "    [9.2, 8.3, 25.5],   # Hour 23\n",
    "])\n",
    "\n",
    "# Normalize using the training scaler\n",
    "high_risk_scaled = scaler.transform(high_risk_window)\n",
    "\n",
    "# Add batch dimension\n",
    "high_risk_scaled = high_risk_scaled[np.newaxis, :, :]\n",
    "\n",
    "# Predict\n",
    "high_risk_pred = model.predict(high_risk_scaled, verbose=0)\n",
    "print(\"=\" * 60)\n",
    "print(\"HIGH RISK SCENARIO\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Features: High stress (8.7-9.5/10), High workload (7.9-8.6/10), Low HRV (24.5-26.2 ms)\")\n",
    "print(f\"Migraine probability: {high_risk_pred[0][0]:.4f} ({100*high_risk_pred[0][0]:.2f}%)\")\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# LOW RISK SCENARIO: Low stress, low workload, high HRV\n",
    "# ============================================================\n",
    "low_risk_window = np.array([\n",
    "    [2.5, 2.5, 65.0],   # Hour 0: Low stress/workload, high HRV\n",
    "    [2.3, 2.4, 64.0],   # Hour 1\n",
    "    [2.4, 2.6, 65.5],   # Hour 2\n",
    "    [2.6, 2.5, 66.0],   # Hour 3\n",
    "    [2.2, 2.3, 64.5],   # Hour 4\n",
    "    [2.4, 2.5, 65.2],   # Hour 5\n",
    "    [2.3, 2.4, 65.8],   # Hour 6\n",
    "    [2.5, 2.6, 64.9],   # Hour 7\n",
    "    [2.4, 2.5, 65.3],   # Hour 8\n",
    "    [2.6, 2.7, 66.1],   # Hour 9\n",
    "    [2.3, 2.4, 64.6],   # Hour 10\n",
    "    [2.4, 2.5, 65.4],   # Hour 11\n",
    "    [2.5, 2.6, 65.9],   # Hour 12\n",
    "    [2.2, 2.3, 64.2],   # Hour 13\n",
    "    [2.3, 2.4, 65.7],   # Hour 14\n",
    "    [2.4, 2.5, 66.0],   # Hour 15\n",
    "    [2.6, 2.7, 65.1],   # Hour 16\n",
    "    [2.3, 2.4, 64.8],   # Hour 17\n",
    "    [2.5, 2.6, 65.5],   # Hour 18\n",
    "    [2.4, 2.5, 66.2],   # Hour 19\n",
    "    [2.2, 2.3, 64.3],   # Hour 20\n",
    "    [2.3, 2.4, 65.6],   # Hour 21\n",
    "    [2.5, 2.6, 65.8],   # Hour 22\n",
    "    [2.4, 2.5, 66.1],   # Hour 23\n",
    "])\n",
    "\n",
    "# Normalize using the training scaler\n",
    "low_risk_scaled = scaler.transform(low_risk_window)\n",
    "\n",
    "# Add batch dimension\n",
    "low_risk_scaled = low_risk_scaled[np.newaxis, :, :]\n",
    "\n",
    "# Predict\n",
    "low_risk_pred = model.predict(low_risk_scaled, verbose=0)\n",
    "print(\"=\" * 60)\n",
    "print(\"LOW RISK SCENARIO\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Features: Low stress (2.2-2.6/10), Low workload (2.3-2.7/10), High HRV (64.0-66.2 ms)\")\n",
    "print(f\"Migraine probability: {low_risk_pred[0][0]:.4f} ({100*low_risk_pred[0][0]:.2f}%)\")\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# MEDIUM RISK SCENARIO: Moderate values\n",
    "# ============================================================\n",
    "medium_risk_window = np.array([\n",
    "    [5.5, 5.5, 45.0],   # Hour 0: Medium everything\n",
    "    [5.3, 5.4, 44.0],   # Hour 1\n",
    "    [5.4, 5.6, 45.5],   # Hour 2\n",
    "    [5.6, 5.5, 46.0],   # Hour 3\n",
    "    [5.2, 5.3, 44.5],   # Hour 4\n",
    "    [5.4, 5.5, 45.2],   # Hour 5\n",
    "    [5.3, 5.4, 45.8],   # Hour 6\n",
    "    [5.5, 5.6, 44.9],   # Hour 7\n",
    "    [5.4, 5.5, 45.3],   # Hour 8\n",
    "    [5.6, 5.7, 46.1],   # Hour 9\n",
    "    [5.3, 5.4, 44.6],   # Hour 10\n",
    "    [5.4, 5.5, 45.4],   # Hour 11\n",
    "    [5.5, 5.6, 45.9],   # Hour 12\n",
    "    [5.2, 5.3, 44.2],   # Hour 13\n",
    "    [5.3, 5.4, 45.7],   # Hour 14\n",
    "    [5.4, 5.5, 46.0],   # Hour 15\n",
    "    [5.6, 5.7, 45.1],   # Hour 16\n",
    "    [5.3, 5.4, 44.8],   # Hour 17\n",
    "    [5.5, 5.6, 45.5],   # Hour 18\n",
    "    [5.4, 5.5, 46.2],   # Hour 19\n",
    "    [5.2, 5.3, 44.3],   # Hour 20\n",
    "    [5.3, 5.4, 45.6],   # Hour 21\n",
    "    [5.5, 5.6, 45.8],   # Hour 22\n",
    "    [5.4, 5.5, 46.1],   # Hour 23\n",
    "])\n",
    "\n",
    "# Normalize using the training scaler\n",
    "medium_risk_scaled = scaler.transform(medium_risk_window)\n",
    "\n",
    "# Add batch dimension\n",
    "medium_risk_scaled = medium_risk_scaled[np.newaxis, :, :]\n",
    "\n",
    "# Predict\n",
    "medium_risk_pred = model.predict(medium_risk_scaled, verbose=0)\n",
    "print(\"=\" * 60)\n",
    "print(\"MEDIUM RISK SCENARIO\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Features: Medium stress (5.2-5.6/10), Medium workload (5.3-5.7/10), Medium HRV (44.0-46.2 ms)\")\n",
    "print(f\"Migraine probability: {medium_risk_pred[0][0]:.4f} ({100*medium_risk_pred[0][0]:.2f}%)\")\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# Summary comparison\n",
    "# ============================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"High risk:   {high_risk_pred[0][0]:.4f}\")\n",
    "print(f\"Medium risk: {medium_risk_pred[0][0]:.4f}\")\n",
    "print(f\"Low risk:    {low_risk_pred[0][0]:.4f}\")\n",
    "print()\n",
    "print(\"Expected: High risk > Medium risk > Low risk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bb8ef5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PROBABILITY DISTRIBUTION DIAGNOSTIC\n",
      "============================================================\n",
      "\n",
      "Training set:\n",
      "  Total: 10063\n",
      "  Mean prob: 0.5686\n",
      "  Std: 0.2066\n",
      "  Min/Max: 0.0903 / 0.9097\n",
      "  % > 0.5: 63.24%\n",
      "  % > 0.2: 95.23%\n",
      "\n",
      "Validation set:\n",
      "  Total: 2156\n",
      "  Mean prob: 0.5680\n",
      "  Std: 0.2005\n",
      "  Min/Max: 0.0918 / 0.9097\n",
      "  % > 0.5: 63.59%\n",
      "  % > 0.2: 95.41%\n",
      "\n",
      "Test set:\n",
      "  Total: 2157\n",
      "  Mean prob: 0.5775\n",
      "  Std: 0.1985\n",
      "  Min/Max: 0.0903 / 0.9097\n",
      "  % > 0.5: 65.14%\n",
      "  % > 0.2: 96.29%\n",
      "\n",
      "============================================================\n",
      "Model: \"model_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_23 (InputLayer)       [(None, 24, 3)]           0         \n",
      "                                                                 \n",
      " masking_22 (Masking)        (None, 24, 3)             0         \n",
      "                                                                 \n",
      " lstm_23 (LSTM)              (None, 24, 64)            17408     \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 24, 64)            0         \n",
      "                                                                 \n",
      " lstm_24 (LSTM)              (None, 32)                12416     \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,913\n",
      "Trainable params: 30,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "158/158 [==============================] - 21s 69ms/step - loss: 0.6256 - auc: 0.0000e+00 - mae: 0.0969 - val_loss: 0.6136 - val_auc: 0.0000e+00 - val_mae: 0.0669 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "158/158 [==============================] - 7s 45ms/step - loss: 0.6114 - auc: 0.0000e+00 - mae: 0.0737 - val_loss: 0.6114 - val_auc: 0.0000e+00 - val_mae: 0.0623 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "158/158 [==============================] - 7s 46ms/step - loss: 0.6084 - auc: 0.0000e+00 - mae: 0.0677 - val_loss: 0.6101 - val_auc: 0.0000e+00 - val_mae: 0.0592 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "158/158 [==============================] - 8s 48ms/step - loss: 0.6077 - auc: 0.0000e+00 - mae: 0.0661 - val_loss: 0.6096 - val_auc: 0.0000e+00 - val_mae: 0.0580 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "158/158 [==============================] - 7s 47ms/step - loss: 0.6068 - auc: 0.0000e+00 - mae: 0.0644 - val_loss: 0.6095 - val_auc: 0.0000e+00 - val_mae: 0.0570 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "158/158 [==============================] - 7s 45ms/step - loss: 0.6066 - auc: 0.0000e+00 - mae: 0.0639 - val_loss: 0.6090 - val_auc: 0.0000e+00 - val_mae: 0.0561 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "158/158 [==============================] - 7s 46ms/step - loss: 0.6066 - auc: 0.0000e+00 - mae: 0.0636 - val_loss: 0.6091 - val_auc: 0.0000e+00 - val_mae: 0.0560 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "158/158 [==============================] - 7s 47ms/step - loss: 0.6063 - auc: 0.0000e+00 - mae: 0.0630 - val_loss: 0.6089 - val_auc: 0.0000e+00 - val_mae: 0.0558 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "158/158 [==============================] - 7s 46ms/step - loss: 0.6061 - auc: 0.0000e+00 - mae: 0.0629 - val_loss: 0.6088 - val_auc: 0.0000e+00 - val_mae: 0.0560 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "158/158 [==============================] - 7s 47ms/step - loss: 0.6061 - auc: 0.0000e+00 - mae: 0.0626 - val_loss: 0.6090 - val_auc: 0.0000e+00 - val_mae: 0.0568 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "158/158 [==============================] - 7s 47ms/step - loss: 0.6061 - auc: 0.0000e+00 - mae: 0.0627 - val_loss: 0.6090 - val_auc: 0.0000e+00 - val_mae: 0.0559 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "158/158 [==============================] - 8s 49ms/step - loss: 0.6059 - auc: 0.0000e+00 - mae: 0.0622 - val_loss: 0.6090 - val_auc: 0.0000e+00 - val_mae: 0.0566 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "158/158 [==============================] - 7s 47ms/step - loss: 0.6059 - auc: 0.0000e+00 - mae: 0.0624 - val_loss: 0.6088 - val_auc: 0.0000e+00 - val_mae: 0.0558 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "157/158 [============================>.] - ETA: 0s - loss: 0.6058 - auc: 0.0000e+00 - mae: 0.0621\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "158/158 [==============================] - 7s 47ms/step - loss: 0.6058 - auc: 0.0000e+00 - mae: 0.0620 - val_loss: 0.6091 - val_auc: 0.0000e+00 - val_mae: 0.0570 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "158/158 [==============================] - 7s 47ms/step - loss: 0.6057 - auc: 0.0000e+00 - mae: 0.0620 - val_loss: 0.6087 - val_auc: 0.0000e+00 - val_mae: 0.0556 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "158/158 [==============================] - 7s 47ms/step - loss: 0.6056 - auc: 0.0000e+00 - mae: 0.0616 - val_loss: 0.6087 - val_auc: 0.0000e+00 - val_mae: 0.0555 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "158/158 [==============================] - 8s 48ms/step - loss: 0.6056 - auc: 0.0000e+00 - mae: 0.0617 - val_loss: 0.6089 - val_auc: 0.0000e+00 - val_mae: 0.0561 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "158/158 [==============================] - 7s 47ms/step - loss: 0.6054 - auc: 0.0000e+00 - mae: 0.0615 - val_loss: 0.6088 - val_auc: 0.0000e+00 - val_mae: 0.0556 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "158/158 [==============================] - 7s 47ms/step - loss: 0.6056 - auc: 0.0000e+00 - mae: 0.0616 - val_loss: 0.6088 - val_auc: 0.0000e+00 - val_mae: 0.0556 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "157/158 [============================>.] - ETA: 0s - loss: 0.6055 - auc: 0.0000e+00 - mae: 0.0616\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "158/158 [==============================] - 8s 49ms/step - loss: 0.6055 - auc: 0.0000e+00 - mae: 0.0616 - val_loss: 0.6086 - val_auc: 0.0000e+00 - val_mae: 0.0553 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "158/158 [==============================] - 8s 48ms/step - loss: 0.6053 - auc: 0.0000e+00 - mae: 0.0611 - val_loss: 0.6086 - val_auc: 0.0000e+00 - val_mae: 0.0556 - lr: 2.5000e-04\n",
      "Epoch 22/100\n",
      "158/158 [==============================] - 7s 47ms/step - loss: 0.6053 - auc: 0.0000e+00 - mae: 0.0613 - val_loss: 0.6086 - val_auc: 0.0000e+00 - val_mae: 0.0555 - lr: 2.5000e-04\n",
      "Epoch 23/100\n",
      "158/158 [==============================] - 7s 47ms/step - loss: 0.6052 - auc: 0.0000e+00 - mae: 0.0610 - val_loss: 0.6087 - val_auc: 0.0000e+00 - val_mae: 0.0557 - lr: 2.5000e-04\n",
      "Epoch 24/100\n",
      "158/158 [==============================] - 7s 47ms/step - loss: 0.6053 - auc: 0.0000e+00 - mae: 0.0612 - val_loss: 0.6086 - val_auc: 0.0000e+00 - val_mae: 0.0555 - lr: 2.5000e-04\n",
      "Epoch 25/100\n",
      "157/158 [============================>.] - ETA: 0s - loss: 0.6052 - auc: 0.0000e+00 - mae: 0.0607\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "158/158 [==============================] - 8s 48ms/step - loss: 0.6052 - auc: 0.0000e+00 - mae: 0.0607 - val_loss: 0.6086 - val_auc: 0.0000e+00 - val_mae: 0.0556 - lr: 2.5000e-04\n",
      "Epoch 26/100\n",
      "158/158 [==============================] - 7s 47ms/step - loss: 0.6052 - auc: 0.0000e+00 - mae: 0.0609 - val_loss: 0.6086 - val_auc: 0.0000e+00 - val_mae: 0.0555 - lr: 1.2500e-04\n",
      "Epoch 27/100\n",
      "158/158 [==============================] - 7s 47ms/step - loss: 0.6053 - auc: 0.0000e+00 - mae: 0.0611 - val_loss: 0.6086 - val_auc: 0.0000e+00 - val_mae: 0.0555 - lr: 1.2500e-04\n",
      "Epoch 28/100\n",
      "158/158 [==============================] - 8s 50ms/step - loss: 0.6052 - auc: 0.0000e+00 - mae: 0.0605 - val_loss: 0.6086 - val_auc: 0.0000e+00 - val_mae: 0.0554 - lr: 1.2500e-04\n",
      "Epoch 29/100\n",
      "158/158 [==============================] - 8s 48ms/step - loss: 0.6051 - auc: 0.0000e+00 - mae: 0.0606 - val_loss: 0.6086 - val_auc: 0.0000e+00 - val_mae: 0.0556 - lr: 1.2500e-04\n",
      "Epoch 30/100\n",
      "157/158 [============================>.] - ETA: 0s - loss: 0.6051 - auc: 0.0000e+00 - mae: 0.0610\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "158/158 [==============================] - 7s 47ms/step - loss: 0.6051 - auc: 0.0000e+00 - mae: 0.0610 - val_loss: 0.6086 - val_auc: 0.0000e+00 - val_mae: 0.0554 - lr: 1.2500e-04\n",
      "Epoch 31/100\n",
      "158/158 [==============================] - 8s 48ms/step - loss: 0.6052 - auc: 0.0000e+00 - mae: 0.0608 - val_loss: 0.6086 - val_auc: 0.0000e+00 - val_mae: 0.0555 - lr: 6.2500e-05\n",
      "Epoch 32/100\n",
      "158/158 [==============================] - 8s 49ms/step - loss: 0.6051 - auc: 0.0000e+00 - mae: 0.0607 - val_loss: 0.6086 - val_auc: 0.0000e+00 - val_mae: 0.0554 - lr: 6.2500e-05\n",
      "Epoch 33/100\n",
      "158/158 [==============================] - 8s 48ms/step - loss: 0.6052 - auc: 0.0000e+00 - mae: 0.0610 - val_loss: 0.6086 - val_auc: 0.0000e+00 - val_mae: 0.0555 - lr: 6.2500e-05\n",
      "Epoch 34/100\n",
      "158/158 [==============================] - 8s 48ms/step - loss: 0.6051 - auc: 0.0000e+00 - mae: 0.0606 - val_loss: 0.6086 - val_auc: 0.0000e+00 - val_mae: 0.0555 - lr: 6.2500e-05\n",
      "Epoch 35/100\n",
      "157/158 [============================>.] - ETA: 0s - loss: 0.6052 - auc: 0.0000e+00 - mae: 0.0608\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "158/158 [==============================] - 8s 48ms/step - loss: 0.6052 - auc: 0.0000e+00 - mae: 0.0608 - val_loss: 0.6086 - val_auc: 0.0000e+00 - val_mae: 0.0554 - lr: 6.2500e-05\n",
      "Epoch 36/100\n",
      "158/158 [==============================] - 8s 51ms/step - loss: 0.6050 - auc: 0.0000e+00 - mae: 0.0602 - val_loss: 0.6086 - val_auc: 0.0000e+00 - val_mae: 0.0555 - lr: 3.1250e-05\n",
      "Epoch 37/100\n",
      "158/158 [==============================] - 8s 48ms/step - loss: 0.6051 - auc: 0.0000e+00 - mae: 0.0605 - val_loss: 0.6086 - val_auc: 0.0000e+00 - val_mae: 0.0555 - lr: 3.1250e-05\n",
      "Epoch 38/100\n",
      "158/158 [==============================] - 8s 48ms/step - loss: 0.6050 - auc: 0.0000e+00 - mae: 0.0604 - val_loss: 0.6086 - val_auc: 0.0000e+00 - val_mae: 0.0554 - lr: 3.1250e-05\n",
      "Epoch 39/100\n",
      "158/158 [==============================] - 8s 48ms/step - loss: 0.6049 - auc: 0.0000e+00 - mae: 0.0601 - val_loss: 0.6086 - val_auc: 0.0000e+00 - val_mae: 0.0554 - lr: 3.1250e-05\n",
      "Epoch 40/100\n",
      "157/158 [============================>.] - ETA: 0s - loss: 0.6050 - auc: 0.0000e+00 - mae: 0.0604\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "158/158 [==============================] - 8s 48ms/step - loss: 0.6050 - auc: 0.0000e+00 - mae: 0.0604 - val_loss: 0.6086 - val_auc: 0.0000e+00 - val_mae: 0.0554 - lr: 3.1250e-05\n",
      "Epoch 41/100\n",
      "158/158 [==============================] - 8s 48ms/step - loss: 0.6050 - auc: 0.0000e+00 - mae: 0.0603 - val_loss: 0.6086 - val_auc: 0.0000e+00 - val_mae: 0.0554 - lr: 1.5625e-05\n",
      "Epoch 42/100\n",
      "158/158 [==============================] - 8s 48ms/step - loss: 0.6052 - auc: 0.0000e+00 - mae: 0.0610 - val_loss: 0.6086 - val_auc: 0.0000e+00 - val_mae: 0.0554 - lr: 1.5625e-05\n",
      "Epoch 43/100\n",
      "158/158 [==============================] - 8s 49ms/step - loss: 0.6052 - auc: 0.0000e+00 - mae: 0.0607 - val_loss: 0.6086 - val_auc: 0.0000e+00 - val_mae: 0.0554 - lr: 1.5625e-05\n",
      "Epoch 44/100\n",
      "158/158 [==============================] - 8s 50ms/step - loss: 0.6050 - auc: 0.0000e+00 - mae: 0.0601 - val_loss: 0.6086 - val_auc: 0.0000e+00 - val_mae: 0.0554 - lr: 1.5625e-05\n",
      "Epoch 45/100\n",
      "157/158 [============================>.] - ETA: 0s - loss: 0.6052 - auc: 0.0000e+00 - mae: 0.0608\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "158/158 [==============================] - 8s 48ms/step - loss: 0.6052 - auc: 0.0000e+00 - mae: 0.0608 - val_loss: 0.6086 - val_auc: 0.0000e+00 - val_mae: 0.0555 - lr: 1.5625e-05\n",
      "Epoch 46/100\n",
      "158/158 [==============================] - 8s 48ms/step - loss: 0.6050 - auc: 0.0000e+00 - mae: 0.0605 - val_loss: 0.6086 - val_auc: 0.0000e+00 - val_mae: 0.0555 - lr: 7.8125e-06\n",
      "Epoch 47/100\n",
      "158/158 [==============================] - 8s 49ms/step - loss: 0.6050 - auc: 0.0000e+00 - mae: 0.0602 - val_loss: 0.6086 - val_auc: 0.0000e+00 - val_mae: 0.0555 - lr: 7.8125e-06\n",
      "Epoch 48/100\n",
      "158/158 [==============================] - 8s 48ms/step - loss: 0.6051 - auc: 0.0000e+00 - mae: 0.0605 - val_loss: 0.6086 - val_auc: 0.0000e+00 - val_mae: 0.0555 - lr: 7.8125e-06\n",
      "Epoch 49/100\n",
      "158/158 [==============================] - 8s 49ms/step - loss: 0.6050 - auc: 0.0000e+00 - mae: 0.0604 - val_loss: 0.6086 - val_auc: 0.0000e+00 - val_mae: 0.0554 - lr: 7.8125e-06\n",
      "Epoch 50/100\n",
      "158/158 [==============================] - ETA: 0s - loss: 0.6049 - auc: 0.0000e+00 - mae: 0.0600\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "158/158 [==============================] - 8s 48ms/step - loss: 0.6049 - auc: 0.0000e+00 - mae: 0.0600 - val_loss: 0.6086 - val_auc: 0.0000e+00 - val_mae: 0.0555 - lr: 7.8125e-06\n",
      "Epoch 51/100\n",
      "158/158 [==============================] - 8s 48ms/step - loss: 0.6052 - auc: 0.0000e+00 - mae: 0.0609 - val_loss: 0.6086 - val_auc: 0.0000e+00 - val_mae: 0.0555 - lr: 3.9063e-06\n",
      "Epoch 52/100\n",
      "158/158 [==============================] - 8s 51ms/step - loss: 0.6051 - auc: 0.0000e+00 - mae: 0.0605 - val_loss: 0.6086 - val_auc: 0.0000e+00 - val_mae: 0.0555 - lr: 3.9063e-06\n",
      "Epoch 53/100\n",
      "158/158 [==============================] - ETA: 0s - loss: 0.6050 - auc: 0.0000e+00 - mae: 0.0604Restoring model weights from the end of the best epoch: 43.\n",
      "158/158 [==============================] - 8s 49ms/step - loss: 0.6050 - auc: 0.0000e+00 - mae: 0.0604 - val_loss: 0.6086 - val_auc: 0.0000e+00 - val_mae: 0.0555 - lr: 3.9063e-06\n",
      "Epoch 53: early stopping\n",
      "\n",
      "============================================================\n",
      "TEST SET EVALUATION\n",
      "============================================================\n",
      "Loss: 0.6077\n",
      "AUC: 0.0000\n",
      "MAE: 0.0565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\peace\\anaconda3\\envs\\junction-cpu\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "HIGH RISK SCENARIO TEST\n",
      "============================================================\n",
      "Predicted probability: 0.8102 (81.0%)\n",
      "\n",
      "✅ Saved lstm_migraine_model.h5 and scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "# train_model.py - FIXED VERSION\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "CSV_PATH = \"synthetic_wsht_weather_migraine_prob_600days_hourly_FIXED.csv\"\n",
    "FEATURES = [\"workload_0_10\", \"stress_0_10\", \"hrv_rmssd_ms\"]\n",
    "TARGET_LABEL = \"migraine_prob_next_hour\"\n",
    "SEQ_LEN = 24\n",
    "RANDOM_SEED = 123\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "df = pd.read_csv(CSV_PATH, parse_dates=[\"timestamp\"])\n",
    "df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "df = df[~df[TARGET_LABEL].isna()].reset_index(drop=True)\n",
    "\n",
    "# ✅ KEEP AS FLOAT - don't convert to int!\n",
    "df[TARGET_LABEL] = df[TARGET_LABEL].astype(\"float32\")\n",
    "\n",
    "def build_sequences(frame, features, seq_len):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(frame[features])\n",
    "    joblib.dump(scaler, \"scaler.pkl\")\n",
    "\n",
    "    feat = scaler.transform(frame[features])\n",
    "    y = frame[TARGET_LABEL].values  # Keep as probabilities\n",
    "    X_list, y_list = [], []\n",
    "    for end in range(seq_len, len(frame)):\n",
    "        start = end - seq_len\n",
    "        X_list.append(feat[start:end, :])\n",
    "        y_list.append(y[end])\n",
    "    return np.array(X_list), np.array(y_list)\n",
    "\n",
    "X, y = build_sequences(df, FEATURES, SEQ_LEN)\n",
    "\n",
    "# Split chronologically\n",
    "N = len(X)\n",
    "train_n = int(N * 0.7)\n",
    "val_n   = int(N * 0.15)\n",
    "X_train, y_train = X[:train_n], y[:train_n]\n",
    "X_val,   y_val   = X[train_n:train_n+val_n], y[train_n:train_n+val_n]\n",
    "X_test,  y_test  = X[train_n+val_n:], y[train_n+val_n:]\n",
    "\n",
    "# Diagnostic: Check probability distribution\n",
    "print(\"=\" * 60)\n",
    "print(\"PROBABILITY DISTRIBUTION DIAGNOSTIC\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, y_split in [(\"Training\", y_train), (\"Validation\", y_val), (\"Test\", y_test)]:\n",
    "    print(f\"\\n{name} set:\")\n",
    "    print(f\"  Total: {len(y_split)}\")\n",
    "    print(f\"  Mean prob: {y_split.mean():.4f}\")\n",
    "    print(f\"  Std: {y_split.std():.4f}\")\n",
    "    print(f\"  Min/Max: {y_split.min():.4f} / {y_split.max():.4f}\")\n",
    "    print(f\"  % > 0.5: {100*(y_split > 0.5).mean():.2f}%\")\n",
    "    print(f\"  % > 0.2: {100*(y_split > 0.2).mean():.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "def make_model(input_len, input_dim):\n",
    "    inputs = keras.Input(shape=(input_len, input_dim))\n",
    "    x = layers.Masking(mask_value=0.0)(inputs)\n",
    "    x = layers.LSTM(64, return_sequences=True)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.LSTM(32)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(32, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(1e-3),\n",
    "        loss=\"binary_crossentropy\",  # Better for probabilities\n",
    "        metrics=[\n",
    "            keras.metrics.AUC(name=\"auc\"),\n",
    "            keras.metrics.MeanAbsoluteError(name=\"mae\")\n",
    "        ],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = make_model(SEQ_LEN, len(FEATURES))\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", \n",
    "        mode=\"min\", \n",
    "        factor=0.5, \n",
    "        patience=5, \n",
    "        verbose=1,\n",
    "        min_lr=1e-6\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", \n",
    "        mode=\"min\", \n",
    "        patience=10, \n",
    "        restore_best_weights=True, \n",
    "        verbose=1\n",
    "    ),\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_results = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Loss: {test_results[0]:.4f}\")\n",
    "print(f\"AUC: {test_results[1]:.4f}\")\n",
    "print(f\"MAE: {test_results[2]:.4f}\")\n",
    "\n",
    "# Test on high-risk scenario\n",
    "high_risk_window = np.array([\n",
    "    [9.5, 8.5, 25.0],\n",
    "    [9.2, 8.3, 26.0],\n",
    "    [9.0, 8.1, 24.5],\n",
    "    [8.8, 8.0, 25.5],\n",
    "    [8.5, 7.9, 26.0],\n",
    "    [9.1, 8.2, 25.2],\n",
    "    [9.3, 8.4, 24.8],\n",
    "    [9.0, 8.3, 25.5],\n",
    "    [8.9, 8.1, 26.1],\n",
    "    [9.2, 8.5, 24.9],\n",
    "    [9.4, 8.6, 25.3],\n",
    "    [9.1, 8.2, 25.8],\n",
    "    [8.7, 7.9, 26.2],\n",
    "    [9.0, 8.1, 25.1],\n",
    "    [9.3, 8.4, 24.7],\n",
    "    [8.9, 8.0, 25.9],\n",
    "    [9.2, 8.3, 25.4],\n",
    "    [9.1, 8.2, 26.0],\n",
    "    [8.8, 7.9, 25.6],\n",
    "    [9.4, 8.5, 24.8],\n",
    "    [9.0, 8.1, 25.3],\n",
    "    [8.9, 8.0, 26.1],\n",
    "    [9.3, 8.4, 25.0],\n",
    "    [9.2, 8.3, 25.5],\n",
    "])\n",
    "\n",
    "scaler = joblib.load(\"scaler.pkl\")\n",
    "high_risk_scaled = scaler.transform(high_risk_window)[np.newaxis, :, :]\n",
    "high_risk_pred = model.predict(high_risk_scaled, verbose=0)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"HIGH RISK SCENARIO TEST\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Predicted probability: {high_risk_pred[0][0]:.4f} ({100*high_risk_pred[0][0]:.1f}%)\")\n",
    "\n",
    "model.save(\"lstm_migraine_model.h5\")\n",
    "print(\"\\n✅ Saved lstm_migraine_model.h5 and scaler.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "junction-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
